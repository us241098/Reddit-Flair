{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import string\n",
    "fields = ['title', 'selftext', 'flair']\n",
    "\n",
    "df = pd.read_csv('reddit_with_flairs.csv', skipinitialspace=True, usecols=fields)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205363, 3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.fillna(\" \")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205142, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['title'].str.split().str.len().lt(55)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203541, 3)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['selftext'].str.split().str.len().lt(400)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You all, cannot'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "expand_contractions(\"Y'all, can't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining selftext and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203541, 4)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"text\"] = df[\"title\"] +\" \"+ df[\"selftext\"]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text']=df['text'].apply(lambda x : expand_contractions(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_URL(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_punct(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_html(x))\n",
    "df['text']=df['text'].apply(lambda x : remove_emoji(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Crocodile in water tiger on land Humans Gays a...\n",
       "1         Samsung Galaxy S9 Will launch Next Month At MW...\n",
       "2         Exposed Kerala madrasas teaching Wahabism Saud...\n",
       "3         How can we start a small service provider firm...\n",
       "4         Fsociety Aadhaar  To generate the password the...\n",
       "                                ...                        \n",
       "205358    How Modi Has Trampled the Founding Idea of Ind...\n",
       "205359    Jaipur Police takes a potshot at Masakali 20 â€˜...\n",
       "205360    When do u think international flights will res...\n",
       "205361                         Kangna Vilayati Song Lyrics \n",
       "205362    What do you guys know about girish nanavati an...\n",
       "Name: text, Length: 203541, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "df_n, df_test = model_selection.train_test_split(\n",
    "        df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df.flair.values\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9d68f39978>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test.flair.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count            162832\n",
       "unique                7\n",
       "top       Non-Political\n",
       "freq              55420\n",
       "Name: flair, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_n.flair.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n.to_csv('reddit_model_train_valid.csv', index=False)\n",
    "df_test.to_csv('reddit_model_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for BERT this will be implemented later in Kaggle because of lack of compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utsav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/utsav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/utsav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/utsav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/utsav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/utsav/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 10\n",
    "BERT_PATH = \"./bert_uncased/\"\n",
    "MODEL_PATH = \"app/model.bin\"\n",
    "TRAINING_FILE = \"reddit_model_train_valid.csv\"\n",
    "TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    "    BERT_PATH,\n",
    "    do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Coronavirus', 'Politics', 'Non-Political', 'AskIndia',\n",
       "       'Policy/Economy', 'Science/Technology', 'Business/Finance'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n",
    "dfx.flair.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for LSTM without pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 180763 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(dfx['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (162832, 512)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(dfx['text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (162832, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y = pd.get_dummies(dfx['flair']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146548, 512) (146548, 7)\n",
      "(16284, 512) (16284, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 512, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 512, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 5,081,107\n",
      "Trainable params: 5,081,107\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 117238 samples, validate on 29310 samples\n",
      "Epoch 1/10\n",
      "117238/117238 [==============================] - 714s 6ms/step - loss: 1.2518 - acc: 0.5493 - val_loss: 1.0478 - val_acc: 0.6309\n",
      "Epoch 2/10\n",
      "117238/117238 [==============================] - 650s 6ms/step - loss: 0.9191 - acc: 0.6767 - val_loss: 0.9731 - val_acc: 0.6531\n",
      "Epoch 3/10\n",
      "117238/117238 [==============================] - 656s 6ms/step - loss: 0.7932 - acc: 0.7212 - val_loss: 1.0033 - val_acc: 0.6442\n",
      "Epoch 4/10\n",
      "117238/117238 [==============================] - 659s 6ms/step - loss: 0.7070 - acc: 0.7493 - val_loss: 1.0261 - val_acc: 0.6478\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=2, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AskIndia</th>\n",
       "      <th>Business/Finance</th>\n",
       "      <th>Coronavirus</th>\n",
       "      <th>Non-Political</th>\n",
       "      <th>Policy/Economy</th>\n",
       "      <th>Politics</th>\n",
       "      <th>Science/Technology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AskIndia  Business/Finance  Coronavirus  Non-Political  Policy/Economy  \\\n",
       "0         0                 0            1              0               0   \n",
       "1         0                 0            0              0               0   \n",
       "2         0                 0            0              0               0   \n",
       "3         0                 0            0              1               0   \n",
       "4         1                 0            0              0               0   \n",
       "5         0                 0            0              0               0   \n",
       "6         0                 0            0              0               1   \n",
       "7         0                 0            0              0               0   \n",
       "8         0                 0            0              0               0   \n",
       "9         0                 0            0              0               0   \n",
       "\n",
       "   Politics  Science/Technology  \n",
       "0         0                   0  \n",
       "1         1                   0  \n",
       "2         1                   0  \n",
       "3         0                   0  \n",
       "4         0                   0  \n",
       "5         1                   0  \n",
       "6         0                   0  \n",
       "7         1                   0  \n",
       "8         1                   0  \n",
       "9         1                   0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = pd.get_dummies(dfx['flair'])\n",
    "Y.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16284/16284 [==============================] - 45s 3ms/step\n",
      "Test set\n",
      "  Loss: 1.002\n",
      "  Accuracy: 0.656\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.002420417215923, 0.6556742815325987]\n"
     ]
    }
   ],
   "source": [
    "print(accr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj=model.predict(X_test)\n",
    "jj=np.argmax(jj,axis=1)\n",
    "\n",
    "Y_true=np.argmax(Y_test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16284,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          AskIndia       0.60      0.63      0.61      2567\n",
      "  Business/Finance       0.64      0.57      0.61      1030\n",
      "       Coronavirus       0.66      0.58      0.62       544\n",
      "     Non-Political       0.65      0.66      0.66      5529\n",
      "    Policy/Economy       0.49      0.41      0.45       907\n",
      "          Politics       0.73      0.78      0.76      4912\n",
      "Science/Technology       0.56      0.32      0.41       795\n",
      "\n",
      "          accuracy                           0.66     16284\n",
      "         macro avg       0.62      0.57      0.59     16284\n",
      "      weighted avg       0.65      0.66      0.65     16284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['AskIndia', 'Business/Finance', 'Coronavirus','Non-Political', 'Policy/Economy', 'Politics', 'Science/Technology']\n",
    "print(classification_report(Y_true, jj, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"lstm_no_glove.json\", \"w\") as json_file:\n",
    "\tjson_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"lstm_no_glove.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.7152285e-03 1.7205245e-04 1.0427540e-04 2.2130255e-02 1.1592682e-03\n",
      "  9.6858370e-01 1.3516496e-04]] Politics\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "new_complaint = ['BJP wants us to see Indian Sonia Gandhi as Italian but Canadian Akshay Kumar as Indian']\n",
    "seq = tokenizer.texts_to_sequences(new_complaint)\n",
    "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(padded)\n",
    "labels = ['AskIndia', 'Business/Finance', 'Coronavirus','Non-Political', 'Policy/Economy', 'Politics', 'Science/Technology']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for LSTM with pretrained embeddings (Code is complete just need to train the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 180763 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(dfx['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "# load in pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(os.path.join('glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_NB_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "  if i < MAX_NB_WORDS:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=MAX_SEQUENCE_LENGTH,\n",
    "  trainable=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 50)           2500000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 500, 50)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 2,561,107\n",
      "Trainable params: 61,107\n",
      "Non-trainable params: 2,500,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (162832, 500)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(dfx['text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (162832, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Y = pd.get_dummies(dfx['flair']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146548, 500) (146548, 7)\n",
      "(16284, 500) (16284, 7)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 117238 samples, validate on 29310 samples\n",
      "Epoch 1/6\n",
      "117238/117238 [==============================] - 374s 3ms/step - loss: 1.3203 - accuracy: 0.5088 - val_loss: 1.1846 - val_accuracy: 0.5584\n",
      "Epoch 2/6\n",
      "117238/117238 [==============================] - 370s 3ms/step - loss: 1.2226 - accuracy: 0.5420 - val_loss: 1.1421 - val_accuracy: 0.5750\n",
      "Epoch 3/6\n",
      "117238/117238 [==============================] - 369s 3ms/step - loss: 1.1874 - accuracy: 0.5552 - val_loss: 1.1206 - val_accuracy: 0.5795\n",
      "Epoch 4/6\n",
      " 32000/117238 [=======>......................] - ETA: 4:08 - loss: 1.1766 - accuracy: 0.5587"
     ]
    }
   ],
   "source": [
    "\n",
    "epochs = 6\n",
    "batch_size = 256\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss', patience=2, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"lstm_glove.json\", \"w\") as json_file:\n",
    "\tjson_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"lstm_glove.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for BERT finetuning (code is complete just need to train the model, we can see in the output that my GPU was insufficient for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class BERTDataset:\n",
    "    def __init__(self, text, flair):\n",
    "        self.text = text\n",
    "        self.flair = flair\n",
    "        self.tokenizer = TOKENIZER\n",
    "        self.max_len = MAX_LEN\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.text[item])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.flair[item], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return nn.CrossEntropyLoss()(outputs, targets.view(-1, 1))\n",
    "\n",
    "\n",
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "\n",
    "    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        ids = d[\"ids\"]\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        mask = d[\"mask\"]\n",
    "        targets = d[\"targets\"]\n",
    "\n",
    "        ids = ids.to(device, dtype=torch.long)\n",
    "        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "        mask = mask.to(device, dtype=torch.long)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            ids = d[\"ids\"]\n",
    "            token_type_ids = d[\"token_type_ids\"]\n",
    "            mask = d[\"mask\"]\n",
    "            targets = d[\"targets\"]\n",
    "\n",
    "            ids = ids.to(device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "            mask = mask.to(device, dtype=torch.long)\n",
    "            targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "            outputs = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BERTBaseUncased(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTBaseUncased, self).__init__()\n",
    "        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out = nn.Linear(768, 7)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, o2 = self.bert(\n",
    "            ids, \n",
    "            attention_mask=mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        bo = self.bert_drop(o2)\n",
    "        output = self.out(bo)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Coronavirus', 'Politics', 'Non-Political', 'AskIndia',\n",
       "       'Policy/Economy', 'Science/Technology', 'Business/Finance'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")\n",
    "dfx.flair.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AskIndia' 'Business/Finance' 'Coronavirus' 'Non-Political'\n",
      " 'Policy/Economy' 'Politics' 'Science/Technology']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "dfx['flair'] = le.fit_transform(dfx.flair.values)\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 3.81 GiB total capacity; 2.56 GiB already allocated; 65.75 MiB free; 2.60 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a0ddcff3027a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-a0ddcff3027a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     )\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mbest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, module, device_ids, output_device, dim)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    223\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \"\"\"\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 3.81 GiB total capacity; 2.56 GiB already allocated; 65.75 MiB free; 2.60 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def run():\n",
    "    df_train, df_valid = model_selection.train_test_split(\n",
    "        dfx,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "        stratify=dfx.flair.values\n",
    "    )\n",
    "\n",
    "    df_train = df_train.reset_index(drop=True)\n",
    "    df_valid = df_valid.reset_index(drop=True)\n",
    "\n",
    "    train_dataset = BERTDataset(\n",
    "        text=df_train.text.values,\n",
    "        flair=df_train.flair.values\n",
    "    )\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_dataset = BERTDataset(\n",
    "        text=df_valid.text.values,\n",
    "        flair=df_valid.flair.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=VALID_BATCH_SIZE,\n",
    "        num_workers=1\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = BERTBaseUncased()\n",
    "    torch.cuda.empty_cache()  # entirely clear all allocated memory\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "\n",
    "    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        outputs, targets = eval_fn(valid_data_loader, model, device)\n",
    "        outputs = np.array(outputs) >= 0.5\n",
    "        accuracy = metrics.accuracy_score(targets, outputs)\n",
    "        print(f\"Accuracy Score = {accuracy}\")\n",
    "        if accuracy > best_accuracy:\n",
    "            torch.save(model.state_dict(), MODEL_PATH)\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
